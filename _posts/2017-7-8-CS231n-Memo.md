# Machine Learning Basic Method
## K-Nearest Neighbor Classifier
- Advantages:
    - Takes no time to train
- Backwards:
    - classifier must store all of the training data
    - computational cost at test time(must calculate distance with all training samples)

The Nearest Neighbor Classifer may sometimes be a good choic in some settings(especially if the data is low-dimensional)

## Linear Classifier
<p align="center">

$$
    f(x_i, W, b) = Wx_i + b
$$

</p>

Where $x_i$ is ith image and it is a column vector(convert a image into a vector, which called stretch pixels into single column).

**Note:** Convolutional Neural Network maps image pixels to scores exactly as Linear Classifier, but the mapping($f$) is more complex and contain s more parameters.

### Bias Trick
Put bias into $W$ as a matrix. Combining $W$ and $b$ into a single matrix that holds both of them by extending the vector $x_i$ with one additional dimension that always holds the constant1.
<p align="center">
    <img src="http://i4.piimg.com/586835/c34af577b3f0888e.png" width="70%">
</p>

## Multiclass Support Vector Machine Loss
<p align="center">

$$
    L_i = \sum_{j\neq y_i}{max(0, s_j - s_{y_i} + \Delta)}
$$

</p>

In summary, SVM only cares the difference between $s_j$ and $s_{y_i}$, the SVM loss function wants the score of the correct class $y_i$ to be larger than the incorrect class scores by as least by $\Delta$.

理解: 加入label为第$y_i$个类，用其他类的的score减去第$y_i$个类的score。如果相减的值加上一个阈值比零还小的话则loss为0。意思在于，优化的目标（方向）是使非label的类的score不仅要比label的score小，还要小到一定的程度。

Loss functin shown as $max(0, -)$ is often called the **hinge loss**. And $max(0, -)^2$ is called as **squared hinge loss(hinge loss SVM or $L_2  SVM$)**.

## An interesting expression of **loss function**
> The loss function quantifies our happiness with predictions on the training set.

## Regularization
Loss function often has two items: **data loss** and **regularization loss**.

**Why we need regularization loss?** Because $W$ is not necessarily unique, there might similar $W$ that correctly classify the examples(使loss function最小化的$W$并不唯一). For example, if some parameters $W$ correctly classify all example, then any multiple of these parameters $\lambda W$ will also give zero loss. The regularization penalty will restrict the value of $W$.

**Note.** Due to the regularization penalty we can never achieve loss of exactly 0.0 on all examples, this would only be possible in the pathological setting of $W = 0$, but $W$ need to be $W > 0$.

## Softmax Classifier
The loss is cross-entropy loss.

信息量
: $I_(X = x_i) = -log(p(X = x_i))$
: 理解其含义为，一个事件发生的概率越大，则它所携带的信息量就越小。

熵
: 对所有可能的结果带来的信息量求期望 $H(X) = - \sum{p(X = x_i)log(p(X = x_i))} $

相对熵（KL散度）
: 是对两个随机分布之间距离的度量

## Reference
- [信息论中的概念解释](http://blog.csdn.net/rtygbwwwerr/article/details/50778098)

<br>

# Convolutional Neural Network
## Concepts
Whats is Neural Network
: 用大量的数据 + 最优化方法而学习到的函数逼近器。
: **Why need lots of training data?** 特征维度很高，参数很多，在高维度的空间中，数据样本之间是很稀疏的，不易学习到pattern，所以需要用更多的数据

Image Classification
: Many other seemingly distinct Computer Vision tasks(such as object detection, segmentation) can be reduced to image classification.<br>

: Our goal is to train/learn a function that maps the pixel valules of an image into confidence scores for each class.

Data-driven Approach
: Using many examples and developing learning algorithms to find rule and pattern of the data.

$L_0, L_1$ and $L_2$ Norm
: - $L_0$范数: 向量中非0的元素的个数
  - $L_1$范数: 向量中各个元素的绝对值之和
  - $L_2$范数: $\sqrt{x_1^2 + x_2^2 + ... + x_n^2}$

$L_1$ and $L_2$ Distance
: Two vector $V_1$ and $V_2$, each vector has $n$ element
: - $L_1$ distance:
    $$
        d_{L_1} = \sum_{i}^{n}{|V_1^i - V_2^i|}
    $$
  - $L_1$ distance:
    $$
        d_{L_2} = \sqrt{\sum_{i}^{n}({V_1^i - V_2^i})^2}
    $$

Batch Normalization(BN)
: Input data is a $N * D$ matrix($N$ is num of samples and $D$ is num of features), compute the empirical mean and variance independently for each dimension.<br>

: **理解:**  BN是一个用来做归一化的层。针对神经网络的数据(n, c, h, w), 在一个mini-batch中，对同一channel的所有bath samples进行归一化处理。<br>

: Usually insert batch normalization after fully connected / convolutional layers and before nonlinearity. For example, FC-BN-tanh-FC-BN-tanh.

: **Note:**  When at test step, BatchNorm layer functions's mean and std are not computed based on the batch. Instead, a single fixed expirical mean of activations during training is used.

: Advantage:
    - Improves gradient flow through the Network
    - Allows higher learning rates
    - Reduces the strong dependence on initialization
    - Acts as a form of regularization in a funny way and slightily reduces the need for dropout maybe

Hyperparameters
: The parameters you must decide before training.
: Hyperparameters to play with:
    - Network architecture
    - Learning rate, its decay schedule and update type(一般的，在训练过程中，学习率会随着时间而衰减)
    - Regularization(L2/dropout strength)

Feature Map size
: Input: $(N, C, H_{in}, W_{in})$
: Output: $(N, C, H_{out}, W_{out})$
: $K$: kernel size
: $P$: padding size
: $S$: stride
: **After Convolution/Maxpooling Layer.**  $H_{out} = (H_{in} - K + 2) / S +1$

## Image Data Preprocessing
It is import to **center your data** by substracting the mean from every feature(In the case of images, every pixel is thought of as a feature). Computin a mean image across the training images and subtracting it from every image.

<p>
---------------------------------------------------------------------------
</p>

- The fundamental of ML or DL is optimization. We model a classification or regression problem as a optimization problem, then build a loss function and optimize it.

- To some extent, the reason why different machine learning methods are different is that the way of build model to express the optimaztion problem are different.
(such as SVM)想的还不是很清楚

- What is softmax?

- Number of Neural Network Layers
    Layer number is how many layers need to be calculated weights. So the input layer is not includeded.

- 一般情况下，一个网络的所有层都是用同一种激活函数

- Finetuning
    We rarely ever train ConvNets from scratch. We often use the netword trained(which is called pre-trained network) on a large dataset(such as Imagenet), and then finetune network on our own data.

- Steps of training a network
    Loop:
        1. **Sample** a batch of data
        2. **Forward pro** it through the graph, get loss
        3. **Backprop** to calculate the gradients
        4. Update the parameters using the gradient



## Training Neural Networks
- Activated Function
    - Why not use sigmoid
        - Saturated neurons "kill" the gradients(Gradient Vanishment)
            在sigmoid函数两侧，梯度接近于0，由于反向传播算法采用链式法则，一处梯度为0，后面的梯度都会为0，参数将无法更新

        - Sigmoid outputs are not zero-centered(不是关于原点中心对称的)

    - Why ReLU(Rectified Linear Unit) is better
        - Advantage
            - Does not saturate (in `+` region)
            - Very computationally efficient
            - Converges much faster than sigmoid/tanh in practice
        - Disadvantage
            - Not zero-centered output
            - When input x < 0(the neural is not activated), the gradient will be 0(Gradient Vanishment)

    - When will ReLU be dead
        - When you initialize the weights at first time, a bad situation is that random weights cause the ReLU neural will never be activated.
        - When you train the network with a large learning rate, it will cause the ReLU neural dead. (Rule of thumb: set Learning rate at 0.01 is a good choice)

- Data Preprocessing
    - 归一化在图像处理中并不常用（详见CS231n的课程笔记）
    - 均值中心化处理在图像处理中很常用

- Weights Initialization

<br>

# Mathematics
Jacobian matrix
: 雅克比矩阵是函数的一阶偏导数以一定方式排列成的矩阵。
    <p align="center">
        <img src="http://i4.piimg.com/586835/3baa83cd6f88391d.png" width="50%">
    </p>

: 例:
    <p align="center">
        <img src="http://i4.piimg.com/586835/a0415fa435a392a0.png" width="50%">
    </p>

Hessian matrix
: 海森矩阵是多变量实值函数$f(x_1, x_2, ... , x_n)$的二阶偏导数组成的矩阵。
    <p align="center">
        <img src="http://i4.piimg.com/586835/6165c3e95ebd46ba.png" width="50%">
    </p>


<br>


# Websites
- [CS231n](http://cs231n.stanford.edu/syllabus.html)

- [Github: Convolution arithmetic](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)

- [知乎专栏:CS231n课程笔记翻译系列](https://zhuanlan.zhihu.com/p/22339097)


<br>

# 个人理解

- 机器学习，是个学习+推断的过程，从数据中学习到的是经验，当进行预测时，系统的输入就是你所获得的证据

- 线性分类器类似于模版匹配(对哪一个地方相应很高)，只是学习到的模版类型比较单一
  神经网络可以学到很多模版

- loss function的作用：衡量分类/回归结果的好坏。从而优化它，找到最优的参数获得最好的分类/回归的性能

- 多分类支持向量机, loss function  
$$
L_i=\sum_{j\ne y_i }{max(0, s_j - s_{y_i} + 1)}
$$

- 为什么使用正则化项？
  - 当有多组参数/权重(w)获得了同样的结果(最小化了loss function)时，我们想要寻出最符合我们要求的
<br><br>

- L2正则化项，使模型尽可能多的利用每一纬的输入

- softmax classifier是logistic的一般化形式
  - softmax functionis  
      $$
      \begin{array}{c}
        f = \frac {e^{s_k}} {\sum_{j}{e^{s_j}}} \\ \\
        s=f(x_i; w)
      \end{array}
      $$
- 步长(learning rate)是一个比较头疼的超参数(选多少？)
- mini batch
    针对于每一次参数的update，不会去计算所有的训练集样本的损失函数，只是从其中取出一部分：Mini-batch Gradient Descent
- 机器学习中的优化问题(凸优化问题)：想象为在山谷中找到最低的点
- sift/HOG特征：统计出边缘的方向的分布图
- 特征字典：真的就有点像python中的dictionary，{每种特征：统计结果}
- 将数学表达式转换为图(graph)表示
