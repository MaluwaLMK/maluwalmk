# Machine Learning Basic Method
## K-Nearest Neighbor Classifier
- Advantages:
    - Takes no time to train
- Backwards:
    - classifier must store all of the training data
    - computational cost at test time(must calculate distance with all training samples)

The Nearest Neighbor Classifer may sometimes be a good choic in some settings(especially if the data is low-dimensional)

## Linear Classifier
<p align="center">

$$
    f(x_i, W, b) = Wx_i + b
$$

</p>

Where $x_i$ is ith image and it is a column vector(convert a image into a vector, which called stretch pixels into single column).

**Note:** Convolutional Neural Network maps image pixels to scores exactly as Linear Classifier, but the mapping($f$) is more complex and contain s more parameters.

### Bias Trick
Put bias into $W$ as a matrix. Combining $W$ and $b$ into a single matrix that holds both of them by extending the vector $x_i$ with one additional dimension that always holds the constant1.
<p align="center">
    <img src="http://i4.piimg.com/586835/c34af577b3f0888e.png" width="70%">
</p>

## Multiclass Support Vector Machine Loss
<p align="center">

$$
    L_i = \sum_{j\neq y_i}{max(0, s_j - s_{y_i} + \Delta)}
$$

</p>

In summary, SVM only cares the difference between $s_j$ and $s_{y_i}$, the SVM loss function wants the score of the correct class $y_i$ to be larger than the incorrect class scores by as least by $\Delta$.

理解: 加入label为第$y_i$个类，用其他类的的score减去第$y_i$个类的score。如果相减的值加上一个阈值比零还小的话则loss为0。意思在于，优化的目标（方向）是使非label的类的score不仅要比label的score小，还要小到一定的程度。

Loss functin shown as $max(0, -)$ is often called the **hinge loss**. And $max(0, -)^2$ is called as **squared hinge loss(hinge loss SVM or $L_2  SVM$)**.

## An interesting expression of **loss function**
> The loss function quantifies our happiness with predictions on the training set.

## Regularization
Loss function often has two items: **data loss** and **regularization loss**.

**Why we need regularization loss?** Because $W$ is not necessarily unique, there might similar $W$ that correctly classify the examples(使loss function最小化的$W$并不唯一). For example, if some parameters $W$ correctly classify all example, then any multiple of these parameters $\lambda W$ will also give zero loss. The regularization penalty will restrict the value of $W$.

**Note.** Due to the regularization penalty we can never achieve loss of exactly 0.0 on all examples, this would only be possible in the pathological setting of $W = 0$, but $W$ need to be $W > 0$.

## Softmax Classifier
The loss is cross-entropy loss.

信息量
: $I_(X = x_i) = -log(p(X = x_i))$
: 理解其含义为，一个事件发生的概率越大，则它所携带的信息量就越小。

熵
: 对所有可能的结果带来的信息量求期望 $H(X) = - \sum{p(X = x_i)log(p(X = x_i))} $

相对熵（KL散度）
: 是对两个随机分布之间距离的度量

## Reference
- [信息论中的概念解释](http://blog.csdn.net/rtygbwwwerr/article/details/50778098)

<br>

# Convolutional Neural Network
## Concepts
Whats is Neural Network
: 用大量的数据 + 最优化方法而学习到的函数逼近器。
: **Why need lots of training data?** 特征维度很高，参数很多，在高维度的空间中，数据样本之间是很稀疏的，不易学习到pattern，所以需要用更多的数据

Image Classification
: Many other seemingly distinct Computer Vision tasks(such as object detection, segmentation) can be reduced to image classification.<br>

: Our goal is to train/learn a function that maps the pixel valules of an image into confidence scores for each class.

Data-driven Approach
: Using many examples and developing learning algorithms to find rule and pattern of the data.

$L_0, L_1$ and $L_2$ Norm
: - $L_0$范数: 向量中非0的元素的个数
  - $L_1$范数: 向量中各个元素的绝对值之和
  - $L_2$范数: $\sqrt{x_1^2 + x_2^2 + ... + x_n^2}$

$L_1$ and $L_2$ Distance
: Two vector $V_1$ and $V_2$, each vector has $n$ element
: - $L_1$ distance:
    $$
        d_{L_1} = \sum_{i}^{n}{|V_1^i - V_2^i|}
    $$
  - $L_1$ distance:
    $$
        d_{L_2} = \sqrt{\sum_{i}^{n}({V_1^i - V_2^i})^2}
    $$

Score Function
: Mapping the raw image pixels to class scores

Loss functions
: Measuring the quality of a particular set of parameters based on how well the induced scored agreed with the ground truth labels in the training data.

Batch Normalization(BN)
: Input data is a $N * D$ matrix($N$ is num of samples and $D$ is num of features), compute the empirical mean and variance independently for each dimension.<br>

: **理解:**  BN是一个用来做归一化的层。针对神经网络的数据(n, c, h, w), 在一个mini-batch中，对同一channel的所有bath samples进行归一化处理。<br>

: Usually insert batch normalization after fully connected / convolutional layers and before nonlinearity. For example, FC-BN-tanh-FC-BN-tanh.

: **Note:**  When at test step, BatchNorm layer functions's mean and std are not computed based on the batch. Instead, a single fixed expirical mean of activations during training is used.

: Advantage:
    - Improves gradient flow through the Network
    - Allows higher learning rates
    - Reduces the strong dependence on initialization
    - Acts as a form of regularization in a funny way and slightily reduces the need for dropout maybe

Hyperparameters
: The parameters you must decide before training.
: Hyperparameters to play with:
    - Network architecture
    - Learning rate, its decay schedule and update type(一般的，在训练过程中，学习率会随着时间而衰减)
    - Regularization(L2/dropout strength)

Feature Map size
: Input: $(N, C, H_{in}, W_{in})$
: Output: $(N, C, H_{out}, W_{out})$
: $K$: kernel size
: $P$: padding size
: $S$: stride
: **After Convolution/Maxpooling Layer.**  $H_{out} = (H_{in} - K + 2) / S +1$

## Image Data Preprocessing
It is import to **center your data** by substracting the mean from every feature(In the case of images, every pixel is thought of as a feature). Computin a mean image across the training images and subtracting it from every image.

## Optimization
The goal of optimization is to find the set of parameters $W$ in weight-space that minimize the loss function.
> Our strategy is to start with random weights and iteratively refine them over time to get lower loss

### Gradient
The gradient of a point is the direction of the greatest rate of increase of the function.

Function $f(x_1, x_2, ... x_n)$, its gradient is,
<p>

$$
\nabla f(x_1, x_2, ..., x_n) = \frac{\partial f}{x_1}{e_1} + \frac{\partial f}{x_2}{e_2} + ... + \frac{\partial f}{x_n}{e_n}
$$

</p>

There are two ways to compute the gradient:
- Numerical gradient: slow, approximate but easy way
- Analytic gradient: fast, exact but more-error-prone


Loss function $L(x, W)$, we update weights $W$ in negative gradient direction.

Take SVM as example,
SVM loss function is
<p align="center">

$$
L = \sum_{j\neq y_i}{[max(0, w_j^Tx_i - w_{y_i}^T + \Delta)]}
$$

$$
\nabla_{w_{y_i}} L = -(\sum_{j\neq y_i}1(w_j^Tx_i - w_{y_i}^T + \Delta > 0))x_i
$$

$$
\nabla_{w_j} L = 1(w_j^Tx_i - w_{y_i}^T + \Delta > 0)x_i
$$

</p>

where $1$ is the indicator function that is one if the condition inside is true or zero otherwise.

**Mini-batch gradient descent.**  In large-scale application, the training data can have on order of millions of examples. Hence, it seems wasteful to computer the full loss function over the entire training set in order to perform only a single parameter update. A very common approach to addressing this challenge is to compute the gradient over **batches** of the training data. <br>

理解：将所有的训练样本计算一次loss再对参数更新一次的方法，在样本数量很多时，是非常花费时间的。一种常用的解决方式是，对一小部分的样本进行计算loss，之后根据grediant去更新weights.

The size of the mini-batch is a hyperparameter but it is not very common to cross-validate it. It is usually based on memory constraints, or set to some value, e.g. 32, 64 or 128. We use powers of 2 in practice because many vectorized operation implementations work faster when their inputs are sized in powers of 2.

### Summary
<p align="center">
    <img src="http://i4.piimg.com/586835/6ef4be5212e67f10.png" width="60%">
</p>

**Information Flow.**  The dataset of pairs of $(x, y)$ is given and fixed. The weights start our as random numbers and can change. During the forward pass the score function computes class score, stored in vector $f$. The loss function contains two components: The data loss between the scores and ground truth; The regularization loss is only a function of the weights. During Gradient Descent, we compute the gradient on the weights and use it to update weights.

## Backpropagation
The key of backpropagation is **chain rule**.

我的理解：一个神经元包括的计算有两个：
1. 线性计算 -> 卷积
2. 非线性计算 -> 激活函数

用数学公式表示就是$f(W, x) = \sigma(z(w, x))$, $\sigma$是激活函数，$z$是卷积。<br>
每个神经元在计算时需要做两件事情，**1. 通过输入值计算输出值**  **2. 根据神经元的线性计算函数和非线性计算函数计算输出对输入的局部梯度**。
E.g.
<p>

$$
f(W,x) = \frac{1}{1 + e^{-(w_0x_0 + w_1x_1 + ... + w_nx_n)}} = \frac{1}{1 + e^{-z}}
$$

$w_0x_0 + w_1x_1 + ... + w_nx_n$ 是线性计算部分（卷积）, $\frac{1}{1 + e^{-z}}$为非线性计算部分（激活）。

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

$$
\frac{d\sigma(z)}{dz} = \frac{e^{-z}}{(1 + e^{-z})^2} = (\frac{1 + e^{-z} - 1}{1 + e^{-z}}) = \sigma(z)(1 - \sigma(z))
$$

$$
\nabla_{w_i}f(W, x) = \frac{df(W, z)}{dz} \cdot \frac{dz}{dx} = \sigma(z)(1 - \sigma(z)) \cdot x_i = f(W, x)(1- f(W, x)) \cdot x_i
$$

</p>

**Note:** In cs231n Course Notes [Backpropagation, Intuitions](http://cs231n.github.io/optimization-2/), Section **Backprop in practice: Staged computation** has a very typical example for understanding **chain rule** and how to implement with code.

## Activation Functions
**Sigmoid.** $\sigma(x) = \frac{1}{1 + e^{-x}}$, $\frac{d\sigma}{dx} = \sigma(x)(1 - \sigma(x))$
- (-) Sigmoids saturate and kill gradients(sigmoid函数饱和并使梯度消失).当x比较大或者比较小时，sigmoid函数很平缓，梯度几乎为0。在反向传播时，局部梯度与神经元输出的梯度相乘，因为局部梯度接近为0，导致相乘的结果也为0，这样会“杀死”梯度，使该神经元以及其之后的神经元都无法进行参数更新，整个网络的参数无法达到最优。

**Tanh.** The tanh is simply a scaled sigmoid neuron, $tanh(x) = 2\sigma(2x) - 1$
- (-) Activations saturate same as Sigmoid.

**ReLU.** $f(x) = max(0, x)$
- (+) Greatly accelerate the convergence of stochastic gradient descent(加速了随机梯度下降的收敛).
- (+) Compared with tanh/sigmoid neuron involve expensive operations(exponentials, etc), ReLU is implemented by simply thresholding a matrix of activations at zero(ReLU的计算更加简单，仅计算与0相比的大小即可).
- (-) ReLU neuron can "die". When a large gradient flowing through a ReLU neuronn, it may cause tje weights update in such a way that the neuron will never activate on any datapoint again. The the gradient will always be zero. And if your learning rate is too higt or the weights initialization is bad, will also cause ReLU neuron "die"(因为在$x < 0$, ReLU的梯度为0， 如果在反向传播时输入的梯度过大，或者learning rate过大，导致$w_{i+1} = w_i - \eta \nabla << 0$, 则$f(x) << 0$，经过ReLU后值为0，该ReLU将不再被激活。另外，如果在参数初始化时不合理，也会导致ReLU不会被激活。

**Leaky ReLU.** Instaed ReLU with a small negative slope(of 0.01, or so.) when $x <0, $$f(x) = 1(x<0)(\alpha x) + 1(x >= 0)(x)$

**Maxout.** $f(x) = max(w_1^T x + b1, w_2^T x + b2)$. ReLU and Leaky ReLU are a special case of this form(for example, for ReLU we have w_1, b_1 = 0)


## Neural Network Architectures
**Num of layers.** When we say N-layer neural network, we do not count the input layer.

**Output layer.** Unlike all layers in a Neural Network, the output layer neurons most commonly do not have an activation function.

**Sizing neural network.** E.g.
<p align="center">
    <img src="http://i4.piimg.com/586835/4713acf28055e10a.png" width="50%">
</p>

This network has $4 + 2 = 6$ neurons(not counting the inputs), $[3 * 4] + [4 * 2] = 20$ weights and $4 + 2 = 6$ biases, for a total of 26 learnable parameters.

<p>
---------------------------------------------------------------------------
</p>

- The fundamental of ML or DL is optimization. We model a classification or regression problem as a optimization problem, then build a loss function and optimize it.

- To some extent, the reason why different machine learning methods are different is that the way of build model to express the optimaztion problem are different.
(such as SVM)想的还不是很清楚

- What is softmax?

- Number of Neural Network Layers
    Layer number is how many layers need to be calculated weights. So the input layer is not includeded.

- 一般情况下，一个网络的所有层都是用同一种激活函数

- Finetuning
    We rarely ever train ConvNets from scratch. We often use the netword trained(which is called pre-trained network) on a large dataset(such as Imagenet), and then finetune network on our own data.

- Steps of training a network
    Loop:
        1. **Sample** a batch of data
        2. **Forward pro** it through the graph, get loss
        3. **Backprop** to calculate the gradients
        4. Update the parameters using the gradient



## Training Neural Networks
- Activated Function
    - Why not use sigmoid
        - Saturated neurons "kill" the gradients(Gradient Vanishment)
            在sigmoid函数两侧，梯度接近于0，由于反向传播算法采用链式法则，一处梯度为0，后面的梯度都会为0，参数将无法更新

        - Sigmoid outputs are not zero-centered(不是关于原点中心对称的)

    - Why ReLU(Rectified Linear Unit) is better
        - Advantage
            - Does not saturate (in `+` region)
            - Very computationally efficient
            - Converges much faster than sigmoid/tanh in practice
        - Disadvantage
            - Not zero-centered output
            - When input x < 0(the neural is not activated), the gradient will be 0(Gradient Vanishment)

    - When will ReLU be dead
        - When you initialize the weights at first time, a bad situation is that random weights cause the ReLU neural will never be activated.
        - When you train the network with a large learning rate, it will cause the ReLU neural dead. (Rule of thumb: set Learning rate at 0.01 is a good choice)

- Data Preprocessing
    - 归一化在图像处理中并不常用（详见CS231n的课程笔记）
    - 均值中心化处理在图像处理中很常用

- Weights Initialization

<br>

# Mathematics
Jacobian matrix
: 雅克比矩阵是函数的一阶偏导数以一定方式排列成的矩阵。
    <p align="center">
        <img src="http://i4.piimg.com/586835/3baa83cd6f88391d.png" width="50%">
    </p>

: 例:
    <p align="center">
        <img src="http://i4.piimg.com/586835/a0415fa435a392a0.png" width="50%">
    </p>

Hessian matrix
: 海森矩阵是多变量实值函数$f(x_1, x_2, ... , x_n)$的二阶偏导数组成的矩阵。
    <p align="center">
        <img src="http://i4.piimg.com/586835/6165c3e95ebd46ba.png" width="50%">
    </p>


<br>


# Websites
- [CS231n](http://cs231n.stanford.edu/syllabus.html)

- [Github: Convolution arithmetic](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)

- [知乎专栏:CS231n课程笔记翻译系列](https://zhuanlan.zhihu.com/p/22339097)


<br>

# 个人理解

- 机器学习，是个学习+推断的过程，从数据中学习到的是经验，当进行预测时，系统的输入就是你所获得的证据

- 线性分类器类似于模版匹配(对哪一个地方相应很高)，只是学习到的模版类型比较单一
  神经网络可以学到很多模版

- loss function的作用：衡量分类/回归结果的好坏。从而优化它，找到最优的参数获得最好的分类/回归的性能

- 多分类支持向量机, loss function  
$$
L_i=\sum_{j\ne y_i }{max(0, s_j - s_{y_i} + 1)}
$$

- 为什么使用正则化项？
  - 当有多组参数/权重(w)获得了同样的结果(最小化了loss function)时，我们想要寻出最符合我们要求的
<br><br>

- L2正则化项，使模型尽可能多的利用每一纬的输入

- softmax classifier是logistic的一般化形式
  - softmax functionis  
      $$
      \begin{array}{c}
        f = \frac {e^{s_k}} {\sum_{j}{e^{s_j}}} \\ \\
        s=f(x_i; w)
      \end{array}
      $$
- 步长(learning rate)是一个比较头疼的超参数(选多少？)
- mini batch
    针对于每一次参数的update，不会去计算所有的训练集样本的损失函数，只是从其中取出一部分：Mini-batch Gradient Descent
- 机器学习中的优化问题(凸优化问题)：想象为在山谷中找到最低的点
- sift/HOG特征：统计出边缘的方向的分布图
- 特征字典：真的就有点像python中的dictionary，{每种特征：统计结果}
- 将数学表达式转换为图(graph)表示
