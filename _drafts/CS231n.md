# Convolutional Neural Network Bacis

- The fundamental of ML or DL is optimization. We model a classification or regression problem as a optimization problem, then build a loss function and optimize it.

- To some extent, the reason why different machine learning methods are different is that the way of build model to express the optimaztion problem are different.
(such as SVM)想的还不是很清楚

- What is softmax?




### CS231n
<http://cs231n.stanford.edu/syllabus.html><br>

小知识点：
- 目前处理的图片都必须是同样大小的
- 主流的方式是使用方形的图片

### 个人理解

- 机器学习，是个学习+推断的过程，从数据中学习到的是经验，当进行预测时，系统的输入就是你所获得的证据

- 线性分类器类似于模版匹配(对哪一个地方相应很高)，只是学习到的模版类型比较单一
  神经网络可以学到很多模版

- loss function的作用：衡量分类/回归结果的好坏。从而优化它，找到最优的参数获得最好的分类/回归的性能

- 多分类支持向量机, loss function  
$$
L_i=\sum_{j\ne y_i }{max(0, s_j - s_{y_i} + 1)}
$$

- 为什么使用正则化项？
  - 当有多组参数/权重(w)获得了同样的结果(最小化了loss function)时，我们想要寻出最符合我们要求的
<br><br>

- L2正则化项，使模型尽可能多的利用每一纬的输入

- softmax classifier是logistic的一般化形式
  - softmax functionis  
      $$
      \begin{array}{c}
        f = \frac {e^{s_k}} {\sum_{j}{e^{s_j}}} \\ \\
        s=f(x_i; w)
      \end{array}
      $$
- 步长(learning rate)是一个比较头疼的超参数(选多少？)
- mini batch
    针对于每一次参数的update，不会去计算所有的训练集样本的损失函数，只是从其中取出一部分：Mini-batch Gradient Descent
- 机器学习中的优化问题(凸优化问题)：想象为在山谷中找到最低的点
- sift/HOG特征：统计出边缘的方向的分布图
- 特征字典：真的就有点像python中的dictionary，{每种特征：统计结果}
- 将数学表达式转换为图(graph)表示
