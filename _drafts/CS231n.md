# Convolutional Neural Network Bacis

- The fundamental of ML or DL is optimization. We model a classification or regression problem as a optimization problem, then build a loss function and optimize it.

- To some extent, the reason why different machine learning methods are different is that the way of build model to express the optimaztion problem are different.
(such as SVM)想的还不是很清楚

- What is softmax?

- Number of Neural Network Layers
    Layer number is how many layers need to be calculated weights. So the input layer is not includeded.

- 一般情况下，一个网络的所有层都是用同一种激活函数

- Finetuning
    We rarely ever train ConvNets from scratch. We often use the netword trained(which is called pre-trained network) on a large dataset(such as Imagenet), and then finetune network on our own data.

- Steps of training a network
    Loop:
        1. **Sample** a batch of data
        2. **Forward pro** it through the graph, get loss
        3. **Backprop** to calculate the gradients
        4. Update the parameters using the gradient

## Training Neural Networks
- Activated Function
    - Why not use sigmoid
        - Saturated neurons "kill" the gradients(Gradient Vanishment)
            在sigmoid函数两侧，梯度接近于0，由于反向传播算法采用链式法则，一处梯度为0，后面的梯度都会为0，参数将无法更新

        - Sigmoid outputs are not zero-centered(不是关于原点中心对称的)

    - Why ReLU(Rectified Linear Unit) is better
        - Advantage
            - Does not saturate (in `+` region)
            - Very computationally efficient
            - Converges much faster than sigmoid/tanh in practice
        - Disadvantage
            - Not zero-centered output
            - When input x < 0(the neural is not activated), the gradient will be 0(Gradient Vanishment)

    - When will ReLU be dead
        - When you initialize the weights at first time, a bad situation is that random weights cause the ReLU neural will never be activated.
        - When you train the network with a large learning rate, it will cause the ReLU neural dead. (Rule of thumb: set Learning rate at 0.01 is a good choice)

- Data Preprocessing
    - 归一化在图像处理中并不常用（详见CS231n的课程笔记）
    - 均值中心化处理在图像处理中很常用


# Mathematics
- Jacobian matrix
    向量对向量的导数

### CS231n
<http://cs231n.stanford.edu/syllabus.html><br>

小知识点：
- 目前处理的图片都必须是同样大小的
- 主流的方式是使用方形的图片

### 个人理解

- 机器学习，是个学习+推断的过程，从数据中学习到的是经验，当进行预测时，系统的输入就是你所获得的证据

- 线性分类器类似于模版匹配(对哪一个地方相应很高)，只是学习到的模版类型比较单一
  神经网络可以学到很多模版

- loss function的作用：衡量分类/回归结果的好坏。从而优化它，找到最优的参数获得最好的分类/回归的性能

- 多分类支持向量机, loss function  
$$
L_i=\sum_{j\ne y_i }{max(0, s_j - s_{y_i} + 1)}
$$

- 为什么使用正则化项？
  - 当有多组参数/权重(w)获得了同样的结果(最小化了loss function)时，我们想要寻出最符合我们要求的
<br><br>

- L2正则化项，使模型尽可能多的利用每一纬的输入

- softmax classifier是logistic的一般化形式
  - softmax functionis  
      $$
      \begin{array}{c}
        f = \frac {e^{s_k}} {\sum_{j}{e^{s_j}}} \\ \\
        s=f(x_i; w)
      \end{array}
      $$
- 步长(learning rate)是一个比较头疼的超参数(选多少？)
- mini batch
    针对于每一次参数的update，不会去计算所有的训练集样本的损失函数，只是从其中取出一部分：Mini-batch Gradient Descent
- 机器学习中的优化问题(凸优化问题)：想象为在山谷中找到最低的点
- sift/HOG特征：统计出边缘的方向的分布图
- 特征字典：真的就有点像python中的dictionary，{每种特征：统计结果}
- 将数学表达式转换为图(graph)表示
